{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Volcanoes on Venus â€” End-to-End Notebook\n",
        "\n",
        "This notebook reproduces the full pipeline on the JARtool Magellan SAR dataset:\n",
        "- Data extraction into chips (optional if already prepared)\n",
        "- Train/Val/Test splits with stratification\n",
        "- Custom CNN training and evaluation\n",
        "- Optimizer comparison (SGD, SGD+Momentum)\n",
        "- Transfer learning with ResNet18 and VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config loaded.\n"
          ]
        }
      ],
      "source": [
        "# Paths and switches\n",
        "DATA_ROOT = \"data_processed\"          # expected final dataset layout\n",
        "INTERMEDIATE_ROOT = \"data_intermediate\"\n",
        "RAW_PACKAGE_DIR = \"package\"           # where the extracted UCI tar contents live\n",
        "RESULTS_DIR = \"results\"\n",
        "\n",
        "# Fast mode: skip raw extraction if you already built data_processed/\n",
        "FAST_MODE = True\n",
        "\n",
        "# Training configs\n",
        "IMG_SIZE_CUSTOM = 32      # for custom CNN\n",
        "IMG_SIZE_PRETRAIN = 224   # for ResNet/VGG\n",
        "BATCH_CUSTOM = 128\n",
        "BATCH_RESNET = 64\n",
        "BATCH_VGG = 32\n",
        "EPOCHS_CUSTOM = 20\n",
        "EPOCHS_PRETRAIN = 12\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "\n",
        "print(\"Config loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, math, random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Optional raw reading and chip extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you already have `data_processed/` prepared, keep `FAST_MODE = True` and skip this section.\n",
        "Otherwise, set `FAST_MODE = False` and run these cells to:\n",
        "- Read `.spr/.sdt` files using Python `vread`\n",
        "- Load experiment chip files in VIEW format\n",
        "- Build `data_intermediate/` arrays and a stratified split under `data_processed/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vread_py ready. Set FAST_MODE=False to use it.\n"
          ]
        }
      ],
      "source": [
        "import struct\n",
        "\n",
        "def vread_py(basename):\n",
        "    '''\n",
        "    Python equivalent of vread.m for VIEW format.\n",
        "    Reads `${basename}.spr` to parse metadata, then reads `${basename}.sdt`.\n",
        "    Returns a numpy array with shape (nr, nc) for 2D arrays.\n",
        "    '''\n",
        "    spr = f\"{basename}.spr\"\n",
        "    sdt = f\"{basename}.sdt\"\n",
        "\n",
        "    with open(spr, \"r\") as f:\n",
        "        # spr layout:\n",
        "        # ndim, nc, junk, junk, nr, junk, junk, type\n",
        "        tokens = f.read().split()\n",
        "        ndim = int(tokens[0])\n",
        "        if ndim != 2:\n",
        "            raise ValueError(\"Only 2D data supported\")\n",
        "        nc = int(tokens[1])\n",
        "        # tokens[2], tokens[3] are junk floats\n",
        "        nr = int(tokens[4])\n",
        "        # tokens[5], tokens[6] are junk floats\n",
        "        dtype_code = int(tokens[7])\n",
        "\n",
        "    if dtype_code == 0:\n",
        "        dtype = np.uint8\n",
        "    elif dtype_code == 2:\n",
        "        dtype = np.int32\n",
        "    elif dtype_code == 3:\n",
        "        dtype = np.float32\n",
        "    elif dtype_code == 5:\n",
        "        dtype = np.float64\n",
        "    else:\n",
        "        raise ValueError(\"Unrecognized data type\")\n",
        "\n",
        "    with open(sdt, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    arr = np.frombuffer(raw, dtype=dtype, count=nr*nc)\n",
        "    arr = arr.reshape((nr, nc))\n",
        "    return arr\n",
        "\n",
        "print(\"vread_py ready. Set FAST_MODE=False to use it.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Split builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split utilities ready.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from PIL import Image\n",
        "\n",
        "def ensure_dir(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def stratified_split(indices, labels, train_frac=0.7, val_frac=0.15, test_frac=0.15, seed=SEED):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx_by_class = {}\n",
        "    for i, y in zip(indices, labels):\n",
        "        idx_by_class.setdefault(int(y), []).append(i)\n",
        "    splits = {\"train\": [], \"val\": [], \"test\": []}\n",
        "    for cls, arr in idx_by_class.items():\n",
        "        rng.shuffle(arr)\n",
        "        n = len(arr)\n",
        "        n_train = int(round(train_frac * n))\n",
        "        n_val = int(round(val_frac * n))\n",
        "        train_idx = arr[:n_train]\n",
        "        val_idx = arr[n_train:n_train+n_val]\n",
        "        test_idx = arr[n_train+n_val:]\n",
        "        splits[\"train\"].extend(train_idx)\n",
        "        splits[\"val\"].extend(val_idx)\n",
        "        splits[\"test\"].extend(test_idx)\n",
        "    return splits\n",
        "\n",
        "def save_png_chips(chips_hw, labels, splits, out_root=\"data_processed\", img_size=32):\n",
        "    '''\n",
        "    chips_hw: (N, H, W) uint8\n",
        "    labels: (N,) 0=non_volcano, 1=volcano\n",
        "    '''\n",
        "    ensure_dir(out_root)\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        for cname in [\"non_volcano\",\"volcano\"]:\n",
        "            ensure_dir(Path(out_root)/split/cname)\n",
        "\n",
        "    for split_name, idx_list in splits.items():\n",
        "        for idx in idx_list:\n",
        "            lab = int(labels[idx])\n",
        "            cname = \"volcano\" if lab==1 else \"non_volcano\"\n",
        "            arr = chips_hw[idx]\n",
        "            img = Image.fromarray(arr, mode=\"L\").resize((img_size, img_size), Image.NEAREST)\n",
        "            fname = f\"chip_{idx:06d}.png\"\n",
        "            img.save(Path(out_root)/split_name/cname/fname)\n",
        "\n",
        "    for split in [\"train\",\"val\",\"test\"]:\n",
        "        n0 = len(list(Path(out_root, split, \"non_volcano\").glob(\"*.png\")))\n",
        "        n1 = len(list(Path(out_root, split, \"volcano\").glob(\"*.png\")))\n",
        "        print(f\"{split}: non_volcano={n0}, volcano={n1}\")\n",
        "\n",
        "print(\"Split utilities ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build or reuse splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAST_MODE=True and data_processed/ exists. Skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "if FAST_MODE and Path(DATA_ROOT).exists():\n",
        "    print(\"FAST_MODE=True and data_processed/ exists. Skipping extraction.\")\n",
        "else:\n",
        "    chips_path = Path(INTERMEDIATE_ROOT, \"chips_all.npy\")\n",
        "    labels_path = Path(INTERMEDIATE_ROOT, \"labels_all.npy\")\n",
        "    if chips_path.exists() and labels_path.exists():\n",
        "        chips_hw = np.load(chips_path)    # (N, 15, 15), uint8\n",
        "        labels = np.load(labels_path)     # (N,), 0 or 1\n",
        "        N = chips_hw.shape[0]\n",
        "        idx = np.arange(N)\n",
        "        splits = stratified_split(idx, labels, 0.7, 0.15, 0.15, seed=SEED)\n",
        "        save_png_chips(chips_hw, labels, splits, out_root=DATA_ROOT, img_size=32)\n",
        "    else:\n",
        "        raise SystemExit(\"No precomputed chips_all.npy and labels_all.npy found. \"\n",
        "                         \"Set FAST_MODE=True to reuse existing data_processed/ or prepare intermediates.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. PyTorch dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataloaders ready.\n"
          ]
        }
      ],
      "source": [
        "def get_transforms(img_size=32):\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    eval_tf = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    return train_tf, eval_tf\n",
        "\n",
        "def _make_weighted_sampler(image_folder_dataset):\n",
        "    targets = [cls for (_, cls) in image_folder_dataset.samples]\n",
        "    targets = np.array(targets)\n",
        "    class_counts = np.bincount(targets)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    sample_weights = class_weights[targets]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=torch.as_tensor(sample_weights, dtype=torch.double),\n",
        "        num_samples=len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "    return sampler, class_counts\n",
        "\n",
        "def get_dataloaders(data_root=DATA_ROOT, img_size=32, batch_size=128, num_workers=2, use_weighted_sampler=True):\n",
        "    train_tf, eval_tf = get_transforms(img_size)\n",
        "    train_ds = datasets.ImageFolder(f\"{data_root}/train\", transform=train_tf)\n",
        "    val_ds   = datasets.ImageFolder(f\"{data_root}/val\",   transform=eval_tf)\n",
        "    test_ds  = datasets.ImageFolder(f\"{data_root}/test\",  transform=eval_tf)\n",
        "\n",
        "    if use_weighted_sampler:\n",
        "        sampler, class_counts = _make_weighted_sampler(train_ds)\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
        "    else:\n",
        "        class_counts = None\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader, val_loader, test_loader, train_ds.class_to_idx, class_counts\n",
        "\n",
        "print(\"Dataloaders ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Custom CNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VolcanoCNN defined.\n"
          ]
        }
      ],
      "source": [
        "class VolcanoCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*8*8, 128), nn.ReLU(inplace=True), nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "print(\"VolcanoCNN defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total_seen = 0.0, 0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss.item()) * imgs.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        total_correct += int((preds == labels).sum().item())\n",
        "        total_seen += labels.size(0)\n",
        "    return total_loss/total_seen, (total_correct/total_seen if total_seen else 0.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total_seen = 0.0, 0, 0\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        total_loss += float(loss.item()) * imgs.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        total_correct += int((preds == labels).sum().item())\n",
        "        total_seen += labels.size(0)\n",
        "    return total_loss/total_seen, (total_correct/total_seen if total_seen else 0.0)\n",
        "\n",
        "def plot_curve(history, title, out_png):\n",
        "    plt.figure()\n",
        "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train_loss\")\n",
        "    plt.plot(history[\"epoch\"], history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(title)\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    Path(out_png).parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(out_png, dpi=200); plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Train custom CNN (Adam, 20 epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class_to_idx: {'non_volcano': 0, 'volcano': 1}\n",
            "class_counts (train): [92447  4305]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m best_path.parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS_CUSTOM+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     tr_loss, tr_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      7\u001b[39m logits = model(imgs)\n\u001b[32m      8\u001b[39m loss = criterion(logits, labels)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m optimizer.step()\n\u001b[32m     11\u001b[39m total_loss += \u001b[38;5;28mfloat\u001b[39m(loss.item()) * imgs.size(\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Venus-Atlas-CNN/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Venus-Atlas-CNN/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Venus-Atlas-CNN/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "RESULTS_DIR = \"results\"\n",
        "Path(RESULTS_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "train_loader, val_loader, test_loader, class_to_idx, class_counts = get_dataloaders(\n",
        "    data_root=DATA_ROOT, img_size=IMG_SIZE_CUSTOM, batch_size=BATCH_CUSTOM, num_workers=2, use_weighted_sampler=True\n",
        ")\n",
        "print(\"class_to_idx:\", class_to_idx)\n",
        "print(\"class_counts (train):\", class_counts)\n",
        "\n",
        "model = VolcanoCNN(num_classes=2).to(device)\n",
        "if class_counts is not None:\n",
        "    non_v, vol = float(class_counts[0]), float(class_counts[1])\n",
        "    weights = torch.tensor([1.0, non_v/vol], dtype=torch.float32, device=device)\n",
        "else:\n",
        "    weights = None\n",
        "criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "best_val_acc = 0.0\n",
        "best_path = Path(RESULTS_DIR)/\"custom_cnn_adam\"/\"best_model.pth\"\n",
        "best_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for epoch in range(1, EPOCHS_CUSTOM+1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={tr_loss:.4f} val_loss={val_loss:.4f} | train_acc={tr_acc:.4f} val_acc={val_acc:.4f}\")\n",
        "    history[\"epoch\"].append(epoch)\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"val_loss\"].append(val_loss)\n",
        "    history[\"train_acc\"].append(tr_acc);   history[\"val_acc\"].append(val_acc)\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "\n",
        "plot_curve(history, \"Custom CNN training\", str(Path(RESULTS_DIR)/\"figs\"/\"custom_loss_curve_adam.png\"))\n",
        "print(\"Best model saved to:\", best_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test evaluation for custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'test_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m model.eval()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m imgs, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[32m      9\u001b[39m         imgs, labels = imgs.to(device), labels.to(device)\n\u001b[32m     10\u001b[39m         logits = model(imgs)\n",
            "\u001b[31mNameError\u001b[39m: name 'test_loader' is not defined"
          ]
        }
      ],
      "source": [
        "best_path = Path(RESULTS_DIR)/\"custom_cnn\"/\"best_model.pth\"\n",
        "model = VolcanoCNN(num_classes=2).to(device)\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "\n",
        "preds_all, labels_all = [], []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        preds_all.append(preds.cpu().numpy())\n",
        "        labels_all.append(labels.cpu().numpy())\n",
        "\n",
        "preds_all = np.concatenate(preds_all)\n",
        "labels_all = np.concatenate(labels_all)\n",
        "\n",
        "acc = (preds_all == labels_all).mean()\n",
        "prec = precision_score(labels_all, preds_all, pos_label=1, zero_division=0)\n",
        "rec  = recall_score(labels_all, preds_all, pos_label=1, zero_division=0)\n",
        "cm   = confusion_matrix(labels_all, preds_all, labels=[0,1])\n",
        "acc, prec, rec, cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cm(cm, class_names, out_path):\n",
        "    fig, ax = plt.subplots(figsize=(4,4))\n",
        "    im = ax.imshow(cm, cmap=\"Blues\")\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),\n",
        "           xticklabels=class_names, yticklabels=class_names,\n",
        "           xlabel=\"Predicted\", ylabel=\"True\", title=\"Confusion Matrix (Test)\")\n",
        "    thresh = cm.max()/2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i,j], \"d\"),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(out_path, dpi=200); plt.close()\n",
        "\n",
        "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
        "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "plot_cm(cm, class_names, str(Path(RESULTS_DIR)/\"figs\"/\"confusion_matrix_custom.png\"))\n",
        "print(\"Saved confusion matrix to results/figs/confusion_matrix_custom.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Optimizer comparison (SGD, SGD+Momentum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_with_optimizer(optimizer_builder, tag):\n",
        "    train_loader, val_loader, _, _, class_counts = get_dataloaders(\n",
        "        data_root=DATA_ROOT, img_size=IMG_SIZE_CUSTOM, batch_size=BATCH_CUSTOM, num_workers=2, use_weighted_sampler=True\n",
        "    )\n",
        "    model = VolcanoCNN(num_classes=2).to(device)\n",
        "    if class_counts is not None:\n",
        "        non_v, vol = float(class_counts[0]), float(class_counts[1])\n",
        "        weights = torch.tensor([1.0, non_v/vol], dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        weights = None\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "    optimizer = optimizer_builder(model)\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_val_acc = 0.0\n",
        "    best_path = Path(RESULTS_DIR)/f\"custom_cnn_{tag}\"/\"best_model.pth\"\n",
        "    best_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, EPOCHS_CUSTOM+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "        print(f\"[{tag}] Epoch {epoch:02d} | train_loss={tr_loss:.4f} val_loss={val_loss:.4f} | train_acc={tr_acc:.4f} val_acc={val_acc:.4f}\")\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"train_loss\"].append(tr_loss); history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(tr_acc);   history[\"val_acc\"].append(val_acc)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "\n",
        "    plot_curve(history, f\"Custom CNN training ({tag})\", str(Path(RESULTS_DIR)/\"figs\"/f\"custom_loss_curve_{tag}.png\"))\n",
        "    print(f\"[{tag}] Best model saved to:\", best_path)\n",
        "\n",
        "# SGD and SGD+Momentum\n",
        "run_with_optimizer(lambda m: optim.SGD(m.parameters(), lr=0.01), \"sgd\")\n",
        "run_with_optimizer(lambda m: optim.SGD(m.parameters(), lr=0.01, momentum=0.9), \"sgdm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Transfer learning: ResNet18 and VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_conv1_with_grayscale_resnet(resnet):\n",
        "    old = resnet.conv1\n",
        "    new = nn.Conv2d(1, old.out_channels, kernel_size=old.kernel_size, stride=old.stride, padding=old.padding, bias=False)\n",
        "    with torch.no_grad():\n",
        "        new.weight.copy_(old.weight.mean(dim=1, keepdim=True))\n",
        "    resnet.conv1 = new\n",
        "    return resnet\n",
        "\n",
        "def replace_first_conv_vgg16(vgg):\n",
        "    first = vgg.features[0]\n",
        "    new = nn.Conv2d(1, first.out_channels, kernel_size=first.kernel_size, stride=first.stride, padding=first.padding, bias=first.bias is not None)\n",
        "    with torch.no_grad():\n",
        "        new.weight.copy_(first.weight.mean(dim=1, keepdim=True))\n",
        "        if first.bias is not None:\n",
        "            new.bias.copy_(first.bias)\n",
        "    vgg.features[0] = new\n",
        "    return vgg\n",
        "\n",
        "def train_backbone(model, tag, img_size, batch_size, epochs=EPOCHS_PRETRAIN, lr=1e-4):\n",
        "    train_loader, val_loader, test_loader, class_to_idx, class_counts = get_dataloaders(\n",
        "        data_root=DATA_ROOT, img_size=img_size, batch_size=batch_size, num_workers=2, use_weighted_sampler=True\n",
        "    )\n",
        "    if class_counts is not None:\n",
        "        non_v, vol = float(class_counts[0]), float(class_counts[1])\n",
        "        weights = torch.tensor([1.0, non_v/vol], dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        weights = None\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "    best_val_acc = 0.0\n",
        "    best_path = Path(RESULTS_DIR)/tag/\"best_model.pth\"\n",
        "    best_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = eval_one_epoch(model, val_loader, criterion, device)\n",
        "        print(f\"[{tag}] Epoch {epoch:02d} | train_loss={tr_loss:.4f} val_loss={val_loss:.4f} | train_acc={tr_acc:.4f} val_acc={val_acc:.4f}\")\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"train_loss\"].append(tr_loss); history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(tr_acc);   history[\"val_acc\"].append(val_acc)\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "\n",
        "    plot_curve(history, f\"{tag} fine-tuning\", str(Path(RESULTS_DIR)/\"figs\"/f\"custom_loss_curve_{tag}.png\"))\n",
        "    print(f\"[{tag}] Best model saved to:\", best_path)\n",
        "\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    preds_all, labels_all = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            logits = model(imgs)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            preds_all.append(preds.cpu().numpy())\n",
        "            labels_all.append(labels.cpu().numpy())\n",
        "    preds_all = np.concatenate(preds_all)\n",
        "    labels_all = np.concatenate(labels_all)\n",
        "\n",
        "    acc = (preds_all == labels_all).mean()\n",
        "    prec = precision_score(labels_all, preds_all, pos_label=1, zero_division=0)\n",
        "    rec  = recall_score(labels_all, preds_all, pos_label=1, zero_division=0)\n",
        "\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm   = confusion_matrix(labels_all, preds_all, labels=[0,1])\n",
        "\n",
        "    idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
        "    class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "    def _plot_cm(cm, class_names, out_path):\n",
        "        fig, ax = plt.subplots(figsize=(4,4))\n",
        "        im = ax.imshow(cm, cmap=\"Blues\")\n",
        "        ax.figure.colorbar(im, ax=ax)\n",
        "        ax.set(xticks=np.arange(len(class_names)), yticks=np.arange(len(class_names)),\n",
        "               xticklabels=class_names, yticklabels=class_names,\n",
        "               xlabel=\"Predicted\", ylabel=\"True\", title=\"Confusion Matrix (Test)\")\n",
        "        thresh = cm.max()/2\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                ax.text(j, i, format(cm[i,j], \"d\"),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i,j] > thresh else \"black\")\n",
        "        fig.tight_layout()\n",
        "        Path(out_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(out_path, dpi=200); plt.close()\n",
        "    _plot_cm(cm, class_names, str(Path(RESULTS_DIR)/\"figs\"/f\"confusion_matrix_{tag}.png\"))\n",
        "    print(f\"[{tag}] Test: acc={acc:.4f} precision={prec:.4f} recall={rec:.4f}\")\n",
        "    return acc, prec, rec\n",
        "\n",
        "# ResNet18\n",
        "resnet18 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "resnet18 = replace_conv1_with_grayscale_resnet(resnet18)\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n",
        "resnet18 = resnet18.to(device)\n",
        "acc_res, prec_res, rec_res = train_backbone(resnet18, tag=\"resnet18\", img_size=IMG_SIZE_PRETRAIN, batch_size=BATCH_RESNET)\n",
        "\n",
        "# VGG16\n",
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
        "vgg16 = replace_first_conv_vgg16(vgg16)\n",
        "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
        "vgg16 = vgg16.to(device)\n",
        "acc_vgg, prec_vgg, rec_vgg = train_backbone(vgg16, tag=\"vgg16\", img_size=IMG_SIZE_PRETRAIN, batch_size=BATCH_VGG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final comparison table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "best_path_cnn = Path(RESULTS_DIR)/\"custom_cnn\"/\"best_model.pth\"\n",
        "cnn = VolcanoCNN(num_classes=2).to(device)\n",
        "cnn.load_state_dict(torch.load(best_path_cnn, map_location=device))\n",
        "preds_all, labels_all = [], []\n",
        "cnn.eval()\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        logits = cnn(imgs)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        preds_all.append(preds.cpu().numpy())\n",
        "        labels_all.append(labels.cpu().numpy())\n",
        "preds_all = np.concatenate(preds_all)\n",
        "labels_all = np.concatenate(labels_all)\n",
        "acc_cnn  = (preds_all == labels_all).mean()\n",
        "prec_cnn = precision_score(labels_all, preds_all, pos_label=1, zero_division=0)\n",
        "rec_cnn  = recall_score(labels_all, preds_all, pos_label=1, zero_division=0)\n",
        "\n",
        "df = pd.DataFrame([\n",
        "    {\"Model\": \"Custom CNN (Adam)\", \"Test Accuracy\": acc_cnn, \"Precision (volc)\": prec_cnn, \"Recall (volc)\": rec_cnn},\n",
        "    {\"Model\": \"ResNet18 (FT)\",     \"Test Accuracy\": acc_res, \"Precision (volc)\": prec_res, \"Recall (volc)\": rec_res},\n",
        "    {\"Model\": \"VGG16 (FT)\",        \"Test Accuracy\": acc_vgg, \"Precision (volc)\": prec_vgg, \"Recall (volc)\": rec_vgg},\n",
        "])\n",
        "\n",
        "import caas_jupyter_tools\n",
        "caas_jupyter_tools.display_dataframe_to_user(\"Final Comparison\", df)\n",
        "\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
